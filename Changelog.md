# Changelog

## [v0.2.0] - 2025-08-08

### 주요 변경 사항 (Major Changes)

-   **핵심 아키텍처 리팩터링 (Core Architecture Refactoring):** 프로젝트의 전반적인 안정성, 성능, 확장성을 개선하기 위해 대대적인 아키텍처 리팩터링을 진행했습니다.

### 세부 변경 내역 (Details)

#### 1. 토크나이저 교체 (`CharTokenizer` -> `SentencePiece`)
-   **문제점:** 기존의 문자(Character) 단위 토크나이저는 한국어의 의미 구조를 학습하기 어려워 모델의 성능과 학습 속도를 저하 시키는 결정적인 원인이었습니다.
-   **개선 사항:**
    -   `SentencePiece` (BPE) 기반의 서브워드(Subword) 토크나이저를 도입하여, 모델이 단어의 의미 단위를 효과적으로 학습할 수 있도록 변경했습니다.
    -   이를 통해 모델의 대화 품질 향상과 학습 속도 개선을 기대할 수 있습니다.
    -   `requirements.txt`에 `sentencepiece` 라이브러리를 추가했습니다.

#### 2. 오프라인 데이터 파이프라인 구축
-   **문제점:** 기존에는 학습 시작 시 매번 실시간으로 데이터를 정제하고 토크나이저 어휘 사전을 구축하여 비효율적이었습니다.
-   **개선 사항:**
    -   `scripts/prepare_data.py` 스크립트를 새로 추가했습니다.
    -   이 스크립트를 통해 사전에 전체 학습 데이터를 기반으로 토크나이저를 미리 학습시키고, 모델 파일(`spm_bpe_8k.model`)로 저장합니다.
    -   학습 시에는 미리 생성된 토크나이저를 불러와 사용하도록 변경하여, 학습 시작 시간을 단축하고 일관성을 확보했습니다.

#### 3. 고급 답변 생성(Sampling) 기능 구현
-   **문제점:** UI에는 Top-k, Top-p, Temperature 등 다양한 답변 생성 옵션이 있었지만, 실제 모델의 `generate` 함수에는 해당 기능이 구현되어 있지 않았습니다.
-   **개선 사항:**
    -   `src/model/transformer.py`의 `generate` 함수를 다시 작성하여, UI의 설정값이 실제 답변 생성 과정에 반영되도록 구현했습니다.
    -   `src/service/service.py`의 `infer` 메소드를 수정하여, 설정된 값을 모델에 전달하도록 연결했습니다.

#### 4. 코드 리팩터링 및 정리
-   **모델 저장/로드:** `src/model/transformer.py`의 모델 저장/로드 함수에서 비효율적인 어휘 사전(vocab) 직접 저장을 제거하고, 모델 설정(config)을 통해 어휘 크기를 관리하도록 변경했습니다.
-   **학습 파이프라인:** `src/training/simple.py`의 코드를 리팩터링하여, 새로운 토크나이저와 오프라인 데이터 파이프라인에 맞춰 동작하도록 단순화하고 명확하게 개선했습니다.

## [v0.2.1] - 2025-08-09

### 세부 변경 내역
- SentencePiece 토크나이저 학습 스크립트에 다중 인코딩 처리와 학습 성공 여부 검증 로깅을 추가함.
- `SentencePieceTokenizer` 클래스를 도입하고 문자 단위 토크나이저 관련 코드를 완전히 제거함.
- 트랜스포머 `generate` 메소드에 Top-k, Top-p, Temperature 샘플링을 적용하고 모델 저장/로드 로직을 단순화함.
- 학습 및 서비스 모듈을 전면 교체하여 GPU 전용 학습 루프, 체크포인트 복구, 예외 처리 로직을 강화함.
- 설정 파일에 `tokenizer_path` 항목을 추가하고 백엔드 오류 처리 로직을 개선함.

## [v0.2.2] - 2025-08-10

### 세부 변경 내역
-   **문제점:** UI에서 초기 설정을 요청할 때 `ChatbotService`에 대응 메소드가 없어 예외가 발생할 수 있었고, 백엔드에서도 빈 응답을 반환할 위험이 있었다.
-   **개선 사항:**
    -   `ChatbotService.get_config` 메소드를 추가하여 UI 초기화 설정을 직접 제공하도록 함.
    -   `WebBackend.get_config`에서 서비스 레이어의 메소드 존재 여부를 검사하고, 설정 딕셔너리 유무에 따라 성공 여부를 반환하도록 방어 로직을 보강함.
    -   `/get_config` 엔드포인트 동작을 검증하는 단위 테스트 `tests/ui/test_backend.py`를 추가함.

## [v0.2.3] - 2025-08-11

### 세부 변경 내역
-   GPU/CPU 자동 선택 및 AMP 활성화 여부를 로그 한 줄로 출력하도록 학습 루프 개선.
-   DataLoader가 CUDA 사용 시 `pin_memory=True`로 설정되고, 모든 텐서와 모델이 동일한 device로 이동하도록 수정.
-   `GradScaler`와 `autocast`를 조건부 적용하여 혼합 정밀도를 안전하게 처리.
-   단일 체크포인트(`training_state.pth`)에 모델, 옵티마, 스케줄러, 스케일러, 에폭, 글로벌 스텝, 베스트 지표, 설정을 저장하고 중단된 지점에서 이어서 학습 가능하도록 구현.
-   `_train_epoch`가 글로벌 스텝과 러닝레이트를 반환하도록 갱신하고, 대응 테스트를 추가.
-   `ChatbotService`에 `auto_tune`, `delete_model`, `MAX_INPUT_LEN` 및 입력 검증 로직을 추가하여 서비스 관련 테스트 안정화.

## [v0.2.4] - 2025-08-12

### 세부 변경 내역
-   **문제점:** 체크포인트가 없거나 손상된 경우 학습이 시작되지 않거나 중단되어 진행 상황을 확인하기 어려웠음.
-   **개선 사항:**
    -   학습 진입 시 체크포인트 존재 여부를 확인하고 `MODE=COLD_START|RESUME, ckpt_path, vocab_size` 형식의 로그를 단일 라인으로 출력.
    -   `tokenizer.vocab_size` 기준으로 임베딩과 출력층을 자동 리사이즈하여 토크나이저 변경 시에도 안정적으로 재학습 가능.
    -   `resume` 플래그 기본값을 `False`로 두고, 체크포인트가 존재할 때만 `True`를 허용하도록 강화.
    -   학습 샘플 수, 에폭/스텝 설정, CUDA 사용 여부, 토크나이저 경로와 어휘 크기를 사전 검증하고 조건 불충족 시 명확한 오류로 중단.
    -   드라이런(dry-run) 1스텝을 수행해 전·후방 전달이 정상 동작하는지 확인하는 안전장치를 추가.
    -   이에 따른 단위 테스트를 `tests/test_train_safety.py`에 추가하고 재시작 검증 테스트를 갱신함.

## [v0.2.5] - 2025-08-13

### 세부 변경 내역
- 서비스 레이어 초기화 시 `setup_logger()` 호출을 추가하여 로그가 항상 생성되도록 함.
- 실행 스크립트에서 `SKIP_CUDA_INSTALL` 환경변수로 CUDA 자동 설치 과정을 조건부 수행하도록 수정.
- `datas/finetune/train.jsonl` 샘플 데이터를 생성하여 데이터 경로 정합성을 확보.
- CLI 사전학습 실행을 통해 로그와 `models/pretrain.pth` 생성 여부를 검증하고, 미세튜닝은 데이터 부족으로 실패(`StopIteration`).

## [v0.2.6] - 2025-08-14

### 세부 변경 내역
-   **문제점:** 서비스 실행 시 로그가 누적되고 데이터/환경 이상이나 Windows/CPU 환경의 DataLoader 문제로 학습 루프에 진입하지 못하는 경우가 있었음.
-   **개선 사항:**
    -   서비스 시작 시 `setup_logger()`로 로그를 강제 초기화하고, 학습 전 프리플라이트 검사(플랫폼 보정, 토크나이저 확인, 데이터 크기 검증)를 수행하여 즉시 실패 원인을 반환하도록 함.
    -   `DataLoader`에 Windows 안전 설정(`num_workers=0`, `pin_memory=False`, `persistent_workers=False`)을 적용하고, 학습 시작 전에 `_dry_run`을 1회 실행하여 AMP 및 데이터로더 문제를 조기에 검출.
    -   학습 로그에 `start_training`, `dataset_size`, `train()` 설정, `dry_run=success`를 출력하여 원인 파악을 용이하게 함.

## [v0.2.7] - 2025-08-15

### 세부 변경 내역
- 체크포인트 존재 여부를 자동 감지하여 추가 학습 시 손실이 이어지도록 `resume` 정책을 개선함.
- 모델 삭제 시 `training_state.pth`와 토크나이저 파일을 보존하도록 예외 조건을 추가함.
- UI 스크립트에서 학습 버튼과 백엔드 API 연결을 점검하여 트리거 동작을 확인함.
