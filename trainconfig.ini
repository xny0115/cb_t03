[generate]
lock_ui = yes
temperature = 0.3
top_p = 0.9
max_new_tokens = 128
repetition_penalty = 1.1
top_k = 0
no_repeat_ngram_size = 0
num_beams = 1
do_sample = yes
seed = auto
stop =

[train]
epochs = 20
batch_size = 48
learning_rate = 0.0002
dropout_ratio = 0.1
grad_clip = 1.0
min_lr = 0.00001
use_mixed_precision = yes
model_dim = 256
ff_dim = 1024
num_heads = 8
num_encoder_layers = 6
num_decoder_layers = 6
num_workers = 6
pin_memory = yes
spm_model_path = models/spm.model
resume = no

[pretrain]
# [train]과 다를 때만 기입 (현재는 공통값 그대로 사용)

[finetune]
# [train]과 다를 때만 기입 (현재는 공통값 그대로 사용)
